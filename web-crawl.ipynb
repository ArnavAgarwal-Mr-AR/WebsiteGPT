{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T06:48:45.828521Z",
     "iopub.status.busy": "2025-05-12T06:48:45.828149Z",
     "iopub.status.idle": "2025-05-12T06:49:01.557417Z",
     "shell.execute_reply": "2025-05-12T06:49:01.556697Z",
     "shell.execute_reply.started": "2025-05-12T06:48:45.828482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U crawl4ai\n",
    "!pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T06:49:01.559257Z",
     "iopub.status.busy": "2025-05-12T06:49:01.559048Z",
     "iopub.status.idle": "2025-05-12T06:49:29.551692Z",
     "shell.execute_reply": "2025-05-12T06:49:29.550710Z",
     "shell.execute_reply.started": "2025-05-12T06:49:01.559238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!crawl4ai-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T06:49:29.555493Z",
     "iopub.status.busy": "2025-05-12T06:49:29.555292Z",
     "iopub.status.idle": "2025-05-12T06:49:36.956305Z",
     "shell.execute_reply": "2025-05-12T06:49:36.955611Z",
     "shell.execute_reply.started": "2025-05-12T06:49:29.555472Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!crawl4ai-doctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T06:49:36.958752Z",
     "iopub.status.busy": "2025-05-12T06:49:36.958288Z",
     "iopub.status.idle": "2025-05-12T06:49:36.966348Z",
     "shell.execute_reply": "2025-05-12T06:49:36.965820Z",
     "shell.execute_reply.started": "2025-05-12T06:49:36.958729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T06:49:36.967281Z",
     "iopub.status.busy": "2025-05-12T06:49:36.967030Z",
     "iopub.status.idle": "2025-05-12T06:49:36.978212Z",
     "shell.execute_reply": "2025-05-12T06:49:36.977560Z",
     "shell.execute_reply.started": "2025-05-12T06:49:36.967259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def safe_title(res):\n",
    "    \"\"\"Return page title or a fallback slug.\"\"\"\n",
    "    if res.metadata and res.metadata.get(\"title\"):\n",
    "        return res.metadata[\"title\"]\n",
    "    return res.url.rstrip(\"/\").split(\"/\")[-1] or res.url\n",
    "\n",
    "def safe_markdown(res):\n",
    "    \"\"\"Return raw markdown as a string (may be None).\"\"\"\n",
    "    md = res.markdown\n",
    "    if md is None:\n",
    "        return \"\"\n",
    "    if isinstance(md, str):\n",
    "        return md\n",
    "    return md.raw_markdown           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from datetime import datetime \n",
    "from urllib.parse import urldefrag\n",
    "from crawl4ai import (\n",
    "    AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode,\n",
    "    MemoryAdaptiveDispatcher\n",
    ")\n",
    "\n",
    "all_pages   = []  \n",
    "all_chunks  = []  \n",
    "\n",
    "async def crawl_recursive_batch(start_urls, max_depth=3, max_concurrent=10):\n",
    "    browser_config = BrowserConfig(headless=True, verbose=False)\n",
    "    run_config = CrawlerRunConfig(\n",
    "        cache_mode=CacheMode.BYPASS,\n",
    "        stream=False\n",
    "    )\n",
    "    dispatcher = MemoryAdaptiveDispatcher(\n",
    "        memory_threshold_percent=70.0,      \n",
    "        check_interval=1.0,                 \n",
    "        max_session_permit=max_concurrent  \n",
    "    )\n",
    "\n",
    "    # Track visited URLs to prevent revisiting and infinite loops (ignoring fragments)\n",
    "    visited = set()\n",
    "    def normalize_url(url):\n",
    "        # Remove fragment (part after #)\n",
    "        return urldefrag(url)[0]\n",
    "    current_urls = set([normalize_url(u) for u in start_urls])\n",
    "\n",
    "    async with AsyncWebCrawler(config=browser_config) as crawler:\n",
    "        for depth in range(max_depth):\n",
    "            print(f\"\\n=== Crawling Depth {depth+1} ===\")\n",
    "            # Only crawl URLs we haven't seen yet (ignoring fragments)\n",
    "            urls_to_crawl = [normalize_url(url) for url in current_urls if normalize_url(url) not in visited]\n",
    "\n",
    "            if not urls_to_crawl:\n",
    "                break\n",
    "\n",
    "            # Batch-crawl all URLs at this depth in parallel\n",
    "            results = await crawler.arun_many(\n",
    "                urls=urls_to_crawl,\n",
    "                config=run_config,\n",
    "                dispatcher=dispatcher\n",
    "            )\n",
    "\n",
    "            next_level_urls = set()\n",
    "\n",
    "            for result in results:\n",
    "                norm_url = normalize_url(result.url)\n",
    "                visited.add(norm_url)  # Mark as visited (no fragment)\n",
    "                if result.success:\n",
    "                    all_pages.append({\n",
    "                        \"title\": safe_title(result),\n",
    "                        \"url\":   result.url,\n",
    "                        \"md\":    safe_markdown(result).strip()\n",
    "                        })\n",
    "                    all_chunks.append(result.markdown.strip())\n",
    "                    print(f\"[OK] {result.url} | Markdown: {len(result.markdown) if result.markdown else 0} chars\")\n",
    "                    # Collect all new internal links for the next depth\n",
    "                    for link in result.links.get(\"internal\", []):\n",
    "                        next_url = normalize_url(link[\"href\"])\n",
    "                        if next_url not in visited:\n",
    "                            next_level_urls.add(next_url)\n",
    "                else:\n",
    "                    print(f\"[ERROR] {result.url}: {result.error_message}\")\n",
    "                    \n",
    "            # Move to the next set of URLs for the next recursion depth\n",
    "            current_urls = next_level_urls\n",
    "        # --- write llms.txt (curated index) ---\n",
    "        with open(\"llms.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"# {start_urls[0]}\\n\")\n",
    "            f.write(f\"> Auto-generated on {datetime.utcnow():%Y-%m-%d}\\n\\n\")\n",
    "            for p in all_pages:\n",
    "                f.write(f\"- [{p['title']}]({p['url']}.md)\\n\")\n",
    "\n",
    "        # --- write llms-full.txt (full text) ---\n",
    "        with open(\"llms-full.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\\n\".join(all_chunks))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(crawl_recursive_batch([\"https://www.example.com/\"], max_depth=3, max_concurrent=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T06:56:13.961702Z",
     "iopub.status.busy": "2025-05-12T06:56:13.961145Z",
     "iopub.status.idle": "2025-05-12T06:56:14.031355Z",
     "shell.execute_reply": "2025-05-12T06:56:14.030550Z",
     "shell.execute_reply.started": "2025-05-12T06:56:13.961674Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = {\n",
    "    \"binary\": {\n",
    "        \"llms.txt\": {\"data\": open(\"llms.txt\",\"rb\").read().decode(), \"mimeType\":\"text/plain\"},\n",
    "        \"llm-full.txt\": {\"data\": open(\"llms-full.txt\",\"rb\").read().decode(), \"mimeType\":\"text/plain\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('train.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
